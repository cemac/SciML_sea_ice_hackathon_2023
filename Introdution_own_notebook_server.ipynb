{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc50c8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/DeB62Nl.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://i.imgur.com/DeB62Nl.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0191bf5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# This is the Leeds SciML Hackathon Tutorial Notebook.    \n",
    "    \n",
    "This notebook covers:\n",
    "    \n",
    "* How to download/access the Hackathon data\n",
    "* Installing python environment (if applicable)\n",
    "* Exploring the data set\n",
    "* Example U-Net model and training\n",
    "* Using [Weights & Biases and connecting with SciMl community]()\n",
    "* Plotting metrics\n",
    "* Evaluating the model Submitting Results     \n",
    "    \n",
    "*please ensure the suitable notebook is chosen for running on **remote/home machine**, [Google CoLab](Introdution_Google_CoLab.ipynb) or [Jupyter99]())*     \n",
    "    \n",
    "## Useful Links \n",
    "    \n",
    "* [Kaggle Competition Page](https://www.kaggle.com/competitions/leeds-sciml-sea-ice-segmentation)\n",
    "* [Weights & Biases SciML Leeds team](https://wandb.ai/sciml-leeds)\n",
    "\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbc37046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/U4amljFGkiw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "# For readability: disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/U4amljFGkiw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3686eb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Data Download\n",
    "\n",
    "**This notebook is for those using their own notebook server**\n",
    "    \n",
    "**If you're using [Google CoLab]() or [Jupyter99]() please used the linked notebooks**\n",
    "    \n",
    "* Create a Kaggle account: If you don't already have a Kaggle account, go to the Kaggle website (https://www.kaggle.com/) and sign up for an account.\n",
    "\n",
    "    \n",
    "* Generate the kaggle.json file: After logging in to Kaggle, go to your account settings page. Scroll down to the section titled \"API\" and click on the \"Create New API Token\" button. This will download a file named kaggle.json to your computer.\n",
    "    \n",
    "\n",
    "* On home laptop place `kaggle.json` if using google colab please follow the colab notebook instructions\n",
    "    \n",
    "You can now run the cell below to download the data     \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a2958e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from kaggle\n",
      "ensure credentials in ~/.kaggle/kaggle.json\n",
      "see\n",
      "https://www.kaggle.com/docs/api \n",
      "for kaggle API information\n",
      "data folder found checking if data already downloaded\n",
      "data already exists exiting\n"
     ]
    }
   ],
   "source": [
    "! ./datadownload.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32f350",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "The data is already split in to a **test** and **train** dataset for you\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1654414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train\n",
      "number of test files is:\n",
      "704\n",
      "number of training file is:\n",
      "4221\n"
     ]
    }
   ],
   "source": [
    "!ls sciml\n",
    "!echo 'number of test files is:'\n",
    "!ls sciml/test/* | wc -l\n",
    "!echo 'number of training file is:'\n",
    "!ls sciml/train/* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced8582",
   "metadata": {},
   "source": [
    "# üöÄ Installing and importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db04d9de-1357-4a3b-a80c-f8e6f938440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries for this notebook\n",
    "\n",
    "#weights and biases\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "#maths and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#machine learning\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import MeanIoU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "#file system\n",
    "import os, glob\n",
    "import tifffile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1886de6c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "The `patch_paths` variables are arrays containing the names of the patch files.\n",
    "Each patch patch has a corresponding SAR image, multispectral image, and label. (Except we've not given you the labels for the test patch paths).\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b633fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Patch20190101_103_1814_720', 'Patch20190101_105_1814_1200',\n",
       "       'Patch20190101_111_600_600', ..., 'Patch20191218_94_480_1080',\n",
       "       'Patch20191218_95_480_1200', 'Patch20191218_96_480_1320'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = 'sciml'\n",
    "train_patch_paths = np.array([os.path.basename(i[:-9]) for i in glob.glob(f'{DATA_DIR}/train/*_sar.tiff')])\n",
    "test_patch_paths = np.array([os.path.basename(i[:-9]) for i in glob.glob(f'{DATA_DIR}/test/*_sar.tiff')])\n",
    "\n",
    "#display patch paths\n",
    "train_patch_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00755e9e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "We use a data generator generates batches of data on-the-fly, reducing the memory requirements and enabling training on large datasets that may not fit entirely into memory.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e5b17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuctions to load in images and generate batches of data\n",
    "\n",
    "\n",
    "def load_img(path):\n",
    "    img = tifffile.imread(path)\n",
    "    return img\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, patch_paths, data_dir, batch_size, split='train'):\n",
    "        self.patch_paths = patch_paths\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_paths = self.patch_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X_sar, X_ms, y = [], [], []\n",
    "\n",
    "        for patch_path in batch_paths:\n",
    "            try:\n",
    "                sar_path = os.path.join(self.data_dir, self.split, patch_path + '_sar.tiff')\n",
    "                ms_path = os.path.join(self.data_dir, self.split, patch_path + '_vis.tiff')\n",
    "                if self.split != 'test':\n",
    "                    label_path = os.path.join(self.data_dir, self.split, patch_path + '_ref.tiff')\n",
    "                    label_arr = load_img(label_path)\n",
    "                    y.append(label_arr)\n",
    " \n",
    "                    sar_arr = load_img(sar_path)\n",
    "                    ms_arr = load_img(ms_path)\n",
    "\n",
    "                    X_sar.append(sar_arr)\n",
    "                    X_ms.append(ms_arr)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        X_sar = np.expand_dims(np.array(X_sar), -1)[:, ::3, ::3, :]\n",
    "        X_ms = np.array(X_ms)\n",
    "        if self.split != 'test':\n",
    "            y = np.expand_dims(np.array(y), -1)\n",
    "            return [X_sar, X_ms], y\n",
    "        else:\n",
    "            return [X_sar, X_ms], [None]*len(X_sar)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6637dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size and train/test generator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_patch_paths, val_patch_paths = train_test_split(train_patch_paths, test_size=0.2, random_state=42)\n",
    "train_generator = DataGenerator(train_patch_paths, DATA_DIR, batch_size)\n",
    "val_generator = DataGenerator(val_patch_paths, DATA_DIR, batch_size)\n",
    "test_generator = DataGenerator(test_patch_paths, DATA_DIR, batch_size, split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9188f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Defining the model    \n",
    "    \n",
    "The [U-Net](https://pyimagesearch.com/2022/02/21/u-net-image-segmentation-in-keras/) model architecture is a convolutional neural network that consists of an encoder and a decoder.\n",
    "The encoder extracts features from the input images, while the decoder upsamples the features to generate the final segmentation map.\n",
    "\n",
    "The model is compiled with the [Adam optimiser](https://www.shiksha.com/online-courses/articles/adam-optimizer-for-stochastic-gradient-descent/) and [binary cross-entropy loss](https://insideaiml.com/blog/BinaryCross-Entropy-1038).\n",
    "During training, the validation data is used to monitor the performance of the model and early stopping is applied to prevent overfitting.\n",
    "\n",
    "# Weight and Biases    \n",
    "    \n",
    "The training progress can be logged using the wandb library, which allows tracking and visualisation of metrics in a web interface.\n",
    "    \n",
    "## ‚úÖ Sign Up\n",
    "\n",
    "Sign up to a free [Weights & Biases account here](https://wandb.ai/signup)\n",
    "\n",
    "[Weights and Biases docs](https://docs.wandb.ai/quickstart)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9f3d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet():\n",
    "    # Input layers\n",
    "    input_sar = Input((240, 240, 1))\n",
    "    input_ms = Input((240, 240, 3))\n",
    "    \n",
    "    # Resizing input_sar\n",
    "    # resized_ms = UpSampling2D(size=(3, 3))(input_ms)\n",
    "\n",
    "    # Concatenate inputs\n",
    "    concat = concatenate([input_sar, input_ms], axis=-1)\n",
    "    \n",
    "    # Encoding path\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(concat)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    \n",
    "    # Bridge\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    \n",
    "    # Decoding path\n",
    "    up6 = UpSampling2D(size=(2, 2))(drop5)\n",
    "    up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(up6)\n",
    "    merge6 = concatenate([drop4, up6], axis=3)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "    \n",
    "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(up7)\n",
    "    merge7 = concatenate([conv3, up7], axis=3)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    # Output layers\n",
    "    output = Conv2D(1, 1, activation = 'sigmoid')(conv7)\n",
    "    output = UpSampling2D(size=(4, 4))(output)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=[input_sar, input_ms], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db75cd17-64a9-4d0f-969b-213044600b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa0ca3",
   "metadata": {},
   "source": [
    "model.fit(train_generator,\n",
    "          validation_data=test_generator,\n",
    "          epochs=10,\n",
    "          use_multiprocessing=True,\n",
    "          workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de23374",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "  \n",
    "# Training the model\n",
    "    \n",
    "if you have signed up for weights and biases then set `wandb_enabled = True`, if not the set to `False`\n",
    "    \n",
    "The cell below will train the model and capture some metrics     \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83f318-0dae-4745-b31e-b202cf3f3028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mh-l-burns\u001b[0m (\u001b[33msciml-leeds\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sciml-leeds/sea-ice-segmentation/runs/stu1f039' target=\"_blank\">spring-water-4</a></strong> to <a href='https://wandb.ai/sciml-leeds/sea-ice-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sciml-leeds/sea-ice-segmentation' target=\"_blank\">https://wandb.ai/sciml-leeds/sea-ice-segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sciml-leeds/sea-ice-segmentation/runs/stu1f039' target=\"_blank\">https://wandb.ai/sciml-leeds/sea-ice-segmentation/runs/stu1f039</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.4280 - accuracy: 0.8223 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 17). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best)... Done. 2.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 410s 12s/step - loss: 0.4280 - accuracy: 0.8223 - val_loss: 0.2314 - val_accuracy: 0.9129\n",
      "Epoch 2/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9121 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 17). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best)... Done. 2.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 403s 11s/step - loss: 0.2293 - accuracy: 0.9121 - val_loss: 0.1839 - val_accuracy: 0.9300\n",
      "Epoch 3/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2034 - accuracy: 0.9238 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 17). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/nfs/earcemac/earhbu/LIFD/Sci-ML_hackathon_2023/wandb/run-20230615_111712-stu1f039/files/model-best)... Done. 2.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 407s 12s/step - loss: 0.2034 - accuracy: 0.9238 - val_loss: 0.1608 - val_accuracy: 0.9387\n",
      "Epoch 4/10\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9296 "
     ]
    }
   ],
   "source": [
    "# Initialize a new W&B run\n",
    "wandb_enabled = True\n",
    "\n",
    "\n",
    "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
    "if wandb_enabled:\n",
    "    wandb.init(entity='sciml-leeds', project='sea-ice-segmentation')\n",
    "    history = model.fit(train_generator,\n",
    "          validation_data=val_generator,\n",
    "          epochs=10,\n",
    "          use_multiprocessing=True,\n",
    "          workers=6,\n",
    "          callbacks=[earlystopper, WandbCallback()])\n",
    "    wandb.finish()\n",
    "else:\n",
    "    history = model.fit(train_generator,\n",
    "          validation_data=val_generator,\n",
    "          epochs=10,\n",
    "          use_multiprocessing=True,\n",
    "          workers=6,\n",
    "          callbacks=[earlystopper])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edeb887",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "\n",
    "# Saving/Loading Model Weights\n",
    "\n",
    "[Keras can save the model](https://www.tensorflow.org/guide/keras/serialization_and_saving) so you only have to train the model once! Here we've save just the model weights to allow results to be seen \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6606e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f1cf1de9400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment as appropriate to save your model or load the tutorial model\n",
    "\n",
    "#model.save_weights('model/tutorial/exampleUNet.ckpt')\n",
    "\n",
    "#model.load_weights('model/tutorial/exampleUNet.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16bbbd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "# Plotting Metrics\n",
    "    \n",
    "Try plotting the different metrics by altering the line \n",
    "\n",
    "```python    \n",
    "metric_to_plot = 'loss' \n",
    "```\n",
    "to \n",
    "```python  \n",
    "metric_to_plot = 'accuracy' \n",
    "```\n",
    "or \n",
    "```python  \n",
    "metric_to_plot = 'mean_io_u' \n",
    "```   \n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec200a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_to_plot = 'mean_io_u' \n",
    "ys = history.history[metric_to_plot]\n",
    "ys_val = history.history['val_'+metric_to_plot]\n",
    "num_epochs = len(ys)\n",
    "plt.title(str(metric_to_plot))\n",
    "plt.plot(range(num_epochs), ys, label='train')\n",
    "plt.plot(range(num_epochs), ys_val, label='val')\n",
    "plt.legend()\n",
    "plt.ylabel(metric_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255f95e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "# Visualization    \n",
    "    \n",
    "Let's dive into the visualization of our sea ice segmentation results using the validation dataset. By running the code snippet provided, we obtain valuable insights into the model's performance and its ability to accurately identify sea ice boundaries.\n",
    "\n",
    "First, the model makes predictions on the validation dataset using the `model.predict()` function. The resulting predictions are stored in the variable preds. These predictions represent the segmentation masks, indicating the presence or absence of sea ice in the images.\n",
    "\n",
    "Next, we extract a single batch of data from the validation generator using `next(iter(val_generator))`. This batch consists of SAR (Synthetic Aperture Radar) images, multispectral (MS) images, and their corresponding ground truth segmentation masks. These components are unpacked into the variables `X_sar_test`, `X_ms_test`, and `y_test`, respectively.\n",
    "\n",
    "Now, we proceed to visualize the results. Within a loop, we iterate over the predictions stored in preds. For each prediction, we create a figure with a grid layout of four subplots.\n",
    "\n",
    "The first subplot, labeled \"SAR Image,\" displays the SAR image from the validation set. SAR imagery provides valuable information about the surface characteristics of sea ice.\n",
    "\n",
    "The second subplot, labeled \"MS Image,\" showcases the multispectral image captured during the validation process. Multispectral data enables us to analyze sea ice from different spectral perspectives.\n",
    "\n",
    "Moving on to the third subplot, titled \"Segmentation Mask,\" we observe the model's predicted segmentation mask. The thresholding operation (`preds[i,:,:,0]>threshold`) helps us visualize the predicted sea ice boundaries clearly.\n",
    "\n",
    "Lastly, the fourth subplot, labeled \"Ground Truth,\" presents the actual ground truth segmentation mask obtained from the validation dataset. This serves as a reference to assess the accuracy of our model's predictions.\n",
    "\n",
    "By carefully examining these subplots, we can compare the model's predictions against the ground truth and gain insights into the performance and limitations of our sea ice segmentation model.\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946b602",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "You need to specify the threshold value, which determines the threshold for classifying pixels as foreground or background. Adjusting this value may affect the performance of the segmentation.\n",
    "\n",
    "```python\n",
    "threshold = <THRESHOLD>\n",
    "```    \n",
    "\n",
    "Where `<THRESHOLD>` is a value between 0 and 1    \n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90184c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cda01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(val_generator,steps=1)\n",
    "\n",
    "(X_sar_test, X_ms_test), y_test = next(iter(val_generator))\n",
    "\n",
    "for i in range(preds.shape[0]):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(X_sar_test[i,:,:,:])\n",
    "    plt.title('SAR Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(X_ms_test[i,:,:,:])\n",
    "    plt.title('MS Image')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(preds[i,:,:,0]>threshold)\n",
    "    plt.title('Segmentation Mask')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(y_test[i,:,:,0])\n",
    "    plt.title('Ground Truth')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa5366",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Things to think about    \n",
    "    \n",
    "* Are there any notable differences between the predicted segmentation mask and the ground truth?\n",
    "* How does adjusting the threshold value impact the visualisation of the segmentation mask?\n",
    "* Can you identify any challenging areas where the model struggles to accurately predict sea ice boundaries?\n",
    "* What potential implications can accurate sea ice segmentation have on environmental research and decision-making processes?\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb65ea0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Generate Submission\n",
    "Ta-da! üìùüéâ Our model is trained and ready, and it's time to run our predictions on the test set and create the submission file.\n",
    "\n",
    "A submission file named 'submission.csv' is created to store the predictions.\n",
    "For each image in the test set, the image ID and run-length encoded pixels are written to the file.\n",
    "\n",
    "Taken from [kaggle example](#ref:https://www.kaggle.com/paulorzp/run-length-encode-and-decode)    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c097836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ref:https://www.kaggle.com/paulorzp/run-length-encode-and-decode.\n",
    "def rle_encode(mask):\n",
    "    '''\n",
    "    mask: numpy array binary mask \n",
    "    1 - mask \n",
    "    0 - background\n",
    "    Returns encoded run length \n",
    "    '''\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# Generate submission DataFrame\n",
    "submission_df = pd.DataFrame(columns=['ImageId', 'EncodedPixels'])\n",
    "\n",
    "for i, patch_path in enumerate(test_patch_paths):\n",
    "    image_id = os.path.basename(patch_path)\n",
    "    mask = predictions[i, :, :, 0] > threshold\n",
    "    encoded_pixels = rle_encode(mask)\n",
    "    submission_df.loc[i] = [image_id, encoded_pixels]\n",
    "\n",
    "# Save submission DataFrame to CSV file\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98f188",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "    \n",
    "The scoring metric will be Intersection over Union (IoU) which will be evaluted from the submission.csv generated\n",
    "    \n",
    "* [Evaluating your Segmentation model](https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2)\n",
    "* [Intersection over Union (IoU)](https://hasty.ai/docs/mp-wiki/metrics/iou-intersection-over-union)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a5248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
